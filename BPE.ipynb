{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85a88158-ba30-4c81-9656-291e9715c792",
   "metadata": {},
   "source": [
    "# Subword Tokenization and Byte-Pair Encoding\n",
    "\n",
    "This assignment's goals are to be able to\n",
    "\n",
    "1. Implement the Byte-Pair Encoding algorithm\n",
    "2. Explain the trade-offs between character-level and word-level tokenization, and how subword tokenization methods like BPE form a compromise.\n",
    "3. Recognize failure conditions in NLP systems related to tokenization choices. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a71f49a-e2d9-4ea6-a555-950d252f662a",
   "metadata": {},
   "source": [
    "The n-gram models we've seen so far have a major flaw: all words (here, space separated sequences of characters!) are assumed to be entirely unrelated as far as the model is concerned! Recall our morphology lecture where I tried to (and hopefully did) convince you that if we want good representations of language (like the ones that help us predict the next word of a sentence) we'd like those representations to encode structural similarities between words.\n",
    "\n",
    "Consider, for example, a hypothetical corpus that contains many instances of the phrase the bigram `(\"write\", \"code\")` (from phrases like \"I need to write code later\") but 0 instances of `(\"rewrite\", \"code\")`. What probability does our model assign the sentence \"I need to rewrite code later\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1447a1d1-7026-47ba-a299-3e2770ed211e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can notate your answer here for your own notes if you'd like!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d755108-512d-4426-aafa-aaa174e30d1d",
   "metadata": {},
   "source": [
    "### Word-Level Warm-Up\n",
    "\n",
    "Our n-gram models are currently what we call *word-level* models: models that have their atomic units as words in a vocabulary. While we like to spell-out our word tokens in our representations, our model treats different words in the vocabulary as unrelated, unstructured units --- elements in the set $V$. In fact, our language models would work just as well if every element in our vocabulary was replaced by a unique integer. In fact, this is often done in practice! Partially to compress our representations (now you just need an array rather than a dictionary, because your keys are now indicies!), and partially because this will become computationally convenient to us when we move into neural network techniques later in the course!\n",
    "\n",
    "To get comfortable doing that, I'll ask you to warm-up by constructing a \"word2idx\" dictionary that maps a word to it's index in the vocabulary with a function `get_word2idx` (which, for now, will be a nice small example vocab!), as well as an `ngram2idxs` function that turns your n-gram tuples into integer tuples!\n",
    "\n",
    "**Python tip of day**: if you want to iterate through an iterable and keep track of both values and indices, the *#pythonic* way to do so is to use `enumerate`, which the following code block demonstrates!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5b4d98-4f7e-4193-a2d8-53b1f6d33166",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [\"the\", \"dog\", \"cat\", \"is\", \"are\", \"dogs\", \"cats\", \"happy\", \"sleepy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33a0e41-8e23-4633-ac44-5bd07a44e91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bad! \n",
    "for i in range(len(vocab)):\n",
    "    print(i, vocab[i])\n",
    "\n",
    "# Do this instead!\n",
    "for i, w in enumerate(vocab):\n",
    "    print(i, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac74fcd-8676-42c4-83d2-3fd2b8d9c18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Mapping, Sequence, Iterable\n",
    "\n",
    "# TODO: complete get_word2idx and ngram2idxs\n",
    "def get_word2idx(vocab : Sequence[str]) -> Mapping[str, int]:\n",
    "    return {}\n",
    "\n",
    "def ngram2idxs(word2idx : Mapping[str, int], ngram : Sequence[str]) -> Sequence[int]:\n",
    "    return ()\n",
    "\n",
    "print(ngram2idxs(get_word2idx(vocab), (\"dog\", \"is\", \"happy\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d6da68-022b-453c-a611-75e4300c1fa9",
   "metadata": {},
   "source": [
    "Now, a hypothetical to consider: Why don't we assign each n-gram to a number to have even more compressed representations! That is, instead of representing `(\"dog\", \"is\", \"happy\")` with `(1, 3, 7)`, why don't we assign it a single number, say, `10`?\n",
    "\n",
    "There are a couple of reasonable answers, but consider how having a structured representation like `(1, 3, 7)` encodes the relationship between different n-grams. Would it be straightforward to implement backoff if we had a vocabulary of n-grams?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa1baaa-9b89-4bf0-9728-72a422643026",
   "metadata": {},
   "source": [
    "### Character-Level Models\n",
    "\n",
    "Now, to an alternative. These n-grams are *word*-level models. What if we fully-committed to subword information text provides (and abandon the nonsense of space-separated words) and build a *character*-level n-gram model. That is, our vocabulary contains every (unicode) character (including spaces!) and we model with characters as our atomic units!\n",
    "\n",
    "If we assume this, tokenization is easy... except for a couple of technical notes. Like our words, to emphasize that there is nothing about the characters themselves (other than their identity) that our models can consider, we will subsititute an integer for the character itself. Since we're working with characters, there are standard integer encodings for characters (in Python 3, [Unicode](https://home.unicode.org/)). the `ord` function maps characters to their unicode values.\n",
    "\n",
    "**Python tip of the day #2**: As you might guess, doing this will make the number of tokens in a corpus very large. Of course, since a string itself is a sequence of characters (in Python, just strings of length 1) this doesn't actually take up much space. However, this simplicity makes it a good place to introduce python [generators](https://docs.python.org/3/glossary.html#term-generator), a special kind of function that allows you to create something iterable (comparable to returning a list you want to for-loop through) but without having to take up the memory required to store every element of that list at the same time. You do this by *generating* the next value as you need it. Because we generate elements one-by-one, these are *iterable* (remember the iterator interface in Java from Data Structures!) but not indexible (you can't access the value at a specific point!). \n",
    "\n",
    "Take a look at the example below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a71b2b-5365-40b6-b250-d269d43496c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"The dog is sleepy\"\n",
    "\n",
    "def char_tokenize(corpus : str) -> Iterable[str]:\n",
    "    for char in corpus:\n",
    "        yield ord(char)\n",
    "\n",
    "def char_tokenize_no_generator(corpus : str) -> Iterable[str]:\n",
    "    out = []\n",
    "    for char in corpus:\n",
    "        out.append(ord(char))\n",
    "    return out\n",
    "\n",
    "print(\"--- with generator\")\n",
    "for token in char_tokenize(data):\n",
    "    print(token)\n",
    "    \n",
    "print(\"--- without generator\")\n",
    "for token in char_tokenize_no_generator(data):\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cbcfdd-1f24-4c72-b31a-d9ea0637aadb",
   "metadata": {},
   "source": [
    "Now there's a bit of an issue with character-level models:\n",
    "\n",
    "Say I train a 5-gram model on words and a 5-gram model on characters. **Which model do you think will form more coherent outputs if you generated from them? How does $n$ need to change to get comparable levels of contextual awareness?**\n",
    "\n",
    "On the other hand, **do we have to worry about out-of-vocabulary tokens in a character-level model?** Trade-offs!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ea8e51-bcfa-433e-be42-005be0e2ff7a",
   "metadata": {},
   "source": [
    "# Byte-Pair Encoding (BPE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb04c2b5-72be-4c58-ab7d-9d462a6cce57",
   "metadata": {},
   "source": [
    "In comes Byte-Pair Encoding, an algorithm from data compression that we repurpose to achieve one goal: Let's make tokens that are smaller than space-separated words, but larger than individual Unicode characters. How does it work: by finding frequent *pairs* of tokens and merging them together into a new, larger token in our vocabulary! We do this until we get our desired number of tokens in our vocabulary, which acts as a parameter to the algorithm. \n",
    "\n",
    "We can then reproduce this sequence of merges on any corpus in order to *tokenize* that text! This is a way to *learn* good subword token representations from a corpus, using the heuristic of \"pairs that appear together often are likely morphemes (i.e., meaningful units!).\" \n",
    "\n",
    "Technical note: As the name implies, we typically do this over bytes instead of characters. To make this conversion simple, we will limit ourselves to the ASCII text encoding, which represents each character with a byte. Conveniently, the first 256 unicode characters are exactly the 256 ASCII characters in the same order! \n",
    "\n",
    "For the rest of this activity, we'll implement the BPE algorithm to build a tokenizer from scratch. It's worth noting that this algorithm is used by big fancy LLMs (i.e., the GPTs that we have some public knowledge about use a BPE variant!). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a877f84e-e8c5-498e-ba9e-530f0cc89ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPETokenizer:\n",
    "    def __init__(self, train_data : str, vocab_size : int, verbose=False):\n",
    "        assert vocab_size > 257 # Our vocab must at least contain each possible \n",
    "                                # byte value as our alphabet, plus the EOW token \n",
    "        self.EOW = 256\n",
    "        self.vocab = [chr(i) for i in range(256)] + [\"<EOW>\"]\n",
    "        self.train_data = self.preprocess(train_data)\n",
    "        self.merges = [] # formatted as (token1, token2, merged_token)\n",
    "\n",
    "        self.train(vocab_size - 257, verbose=verbose)\n",
    "        \n",
    "    def preprocess(self, train_data : str) -> Sequence[Sequence[int]]:\n",
    "        \"\"\" Convert a string into a integer sequence AND split by word\n",
    "            to ensure no cross-word merges. \n",
    "        \"\"\"\n",
    "        return [([ord(l) for l in w] + [self.EOW]) \n",
    "                for w in train_data.split()]\n",
    "    \n",
    "    def train(self, num_merges : int, verbose=False):\n",
    "        for i in range(num_merges):\n",
    "            # TODO: Get the most frequent bigram\n",
    "            max_bigram = ()\n",
    "            \n",
    "            # TODO: Create a new vocab item for the bigram\n",
    "            new_idx = None\n",
    "\n",
    "            # Print a message for each merge if verbose mode is on\n",
    "            # Good for debugging or understanding what's happening!\n",
    "            if verbose: print(\"Merge {}: {}, {} -> {} ({})\".format(\n",
    "                i, max_bigram, new_idx, self.vocab[new_idx]))\n",
    "            \n",
    "            # TODO: Create and store a new merge rule\n",
    "            new_merge_rule = ()\n",
    "\n",
    "            self.train_data = self.merge(new_merge_rule, self.train_data)\n",
    "            \n",
    "    def get_counts(self) -> Mapping[Sequence[int], int]:\n",
    "        # TODO get bigram counts given current tokenization\n",
    "        return {}\n",
    "\n",
    "    def merge(self, merge_rule : Sequence[int], \n",
    "              data : Sequence[int]) -> Sequence[int]:\n",
    "        return []\n",
    "\n",
    "    def encode(self, data : str) -> Sequence[int]:\n",
    "        data = self.preprocess(data)\n",
    "        for merge_rule in self.merges:\n",
    "            data = self.merge(merge_rule, data)\n",
    "        return [i for w in data for i in w]\n",
    "\n",
    "    def decode(self, data : Sequence[int]) -> str:\n",
    "        return \"\".join((self.vocab[i] for i in data))\n",
    "        \n",
    "    def decode_with_splits(self, data : Sequence[int]) -> str:\n",
    "        return \"_\".join((self.vocab[i] for i in data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d01b13a-123d-4802-b51d-3815599bc0bb",
   "metadata": {},
   "source": [
    "Here is the implementation order I recommend:\n",
    "\n",
    "1. You have `__init__`, `preprocess`, `decode` and `decode_with_splits` given to you. Get an idea of how the given code is structured, though for activities feel free to modify the given structure as you see fit.\n",
    "2. Look at the provided method signatures for `get_counts` (which counts the number of adjacent pairs in the training data) and `merge`(which replaces pairs of adjacent tokens with a new, merged token).\n",
    "3. Write `train` referencing the pseudocode from the reading and provided comments. \n",
    "4. Implement `get_counts` (referencing your n-gram counting code from before! it'll be similar!)\n",
    "5. Implement `merge`\n",
    "6. Test to make sure `train`, `get_counts`, and `merge` work correctly.\n",
    "7. Implement `encode`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca1f888-2e88-4c17-9067-6cf387d16fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mimic the example counts from J&M\n",
    "test_string = \"low low low low low lowest lowest newer newer newer newer newer newer wider wider wider new new\"\n",
    "tokenizer = BPETokenizer(test_string, 257 + 4, verbose=True)\n",
    "\n",
    "encoded = tokenizer.encode(\"newfoundlander\")\n",
    "print(encoded)\n",
    "print(tokenizer.decode(encoded))\n",
    "\n",
    "# See the subword pieces the model chose!\n",
    "print(tokenizer.decode_with_splits(encoded))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f42783-7b36-48b1-ae2b-6d2bdc7f023e",
   "metadata": {},
   "source": [
    "If all goes well, this should match the example from the textbook, with merges, in order, \"er\", \"er<EOW>\", \"ne\", and \"new\". If you up the final requested vocab size, you should get further merges you can check against the reading!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a81768a-268e-4916-8c98-3ab26d5e66ed",
   "metadata": {},
   "source": [
    "**Bonus: Training on a real corpus!**\n",
    "\n",
    "Given time, you can train your BPE tokenizer with various sizes on a \"real\" corpus, like (our classic!) Austen's Emma!\n",
    "\n",
    "This may take a little bit to run: your implementation is meant to be readable, not fast (and I don't expect you to do any optimizations!), but it's worth noting that as given the algorithm is $O(kn)$ where $k$ is the number of merges/vocab size and $n$ is the length of the data in bytes! \n",
    "\n",
    "If you'd like to look at a larger corpus (and have some spare time!), you can run this code on the Brown Corpus or even something larger. This is a real, working BPE implementation! \n",
    "\n",
    "It may be helpful to notice what kinds of tokens are formed by this process. Do all of them seem morphologically legitimate? What kinds of errors can you imagine a model would make if these were it's atomic units?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224684c0-ada6-47e7-99fb-2c5cf6eff2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"austen-emma.txt\") as emma_f:\n",
    "    emma = emma_f.read()\n",
    "\n",
    "EmmaTokenizer = BPETokenizer(emma, 557)\n",
    "\n",
    "# Play around with tokenization here! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d457f52-01d2-4c3b-a4b7-87c48e254edc",
   "metadata": {},
   "source": [
    "**Bonus 2: Working with a \"real\" tokenizer. **\n",
    "\n",
    "Now that you understand how BPE works, it might be worth messing around a bit with a \"real\" BPE tokenizer --- that of GPT2, the predecessor the GPTs that power ChatGPT and it's ilk! \n",
    "\n",
    "Below is a snippet of code that will let you tokenize things using GPT-2's tokenizer to see how words are actually decomposed. Note that the strange symbol that prefaces most words is the equivalent of our `<EOW>` token, except it marks the beginning and not the end of words (what is, in some sense, an arbitrary choice!). \n",
    "\n",
    "This code does use a library I haven't asked you to install yet, so if you'd like to test this out, you can install the (HuggingFace (HF) Transformers library)[https://huggingface.co/docs/transformers/en/index] which has an implementation of the GPT-2 tokenizer with a pretrained model available. The next cell prompts you to install it, and afterward you may need to restart your kernel to import transformers.\n",
    "\n",
    "You could also move to something more fancy, like GPT3/GPT4/etc. tokenizers that you can test using (OpenAI's tokenization demo)[https://platform.openai.com/tokenizer]. These aren't running locally like they would be with HF's implementations, but they will suffice for your exploration!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3b828c-b69c-48af-a122-b027a300268a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e901a5-4aaf-4953-89ee-fb2f7894f703",
   "metadata": {},
   "source": [
    "While you explore, I recommend looking at...\n",
    "\n",
    "1. Multi-digit numbers (Would an LM trained on these have a great representation of numbers?)\n",
    "2. Morphologically complex words (Are these linguistically sensible?)\n",
    "3. [\" SolidGoldMagikarp\"](https://arxiv.org/abs/2405.05417)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a05ae1-a735-42aa-a963-c373795cbcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the GPT2 Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "\n",
    "# Change the example sentence!\n",
    "example = \"This is an example to see when words are decomposed into subword pieces\"\n",
    "tokenizer.tokenize(example)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
